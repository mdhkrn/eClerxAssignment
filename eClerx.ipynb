{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Followed\n",
    "1. Extract the company name\n",
    "2. Pick a company and work on it.\n",
    "3. Take all the inbound tweets which are first in the conversation. (i.e. in_response_to is null) and classify them into various topics using LDA\n",
    "4. Find the topic for each tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "twcs = pd.read_csv('twcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the tweets which are inbound and extract the company using tweet 'text' - This gives us ~7.5 lakh tweets\n",
    "ibnd = twcs.query('inbound==True and in_response_to_tweet_id.isnull()').reset_index(drop=True)\n",
    "ibnd['company'] = ibnd['text'].apply(lambda x : None if len(re.findall(\"@[0-9a-zA-Z_]+\", x))==0 else re.findall(\"@[0-9a-zA-Z_]+\", x)[0][1:])\n",
    "\n",
    "#For tweets whose company is null from above(~50k tweets), find their company by using the below logic.\n",
    "#Logic: find the tweets which have respond to this tweet and take the author_id of the first tweet. \n",
    "\n",
    "ibnd['response_tweet_id_1'] = ibnd.apply(lambda x : None if pd.notnull(x['company']) or pd.isnull(x['response_tweet_id']) else [int(i) for i in x['response_tweet_id'].split(',')][0], axis=1  )\n",
    "ibnd = ibnd.merge(twcs[['tweet_id', 'author_id']], how='left', left_on='response_tweet_id_1', right_on='tweet_id')\n",
    "ibnd['company'] = ibnd.apply(lambda x : x['company'] if pd.notnull(x['company']) else x['author_id_y'], axis=1)\n",
    "\n",
    "#Drop the tweets whose company can't be found even after apply the above logic. <500 tweets are dropped\n",
    "ibnd = ibnd[pd.notnull(ibnd['company'])][['text', 'company']]\n",
    "\n",
    "#Making a copy of ibnd\n",
    "ibnd_copy = ibnd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the tweets for Apple and find the topics\n",
    "\n",
    "#Select Apple tweets\n",
    "ibnd = ibnd_copy.query('company==\"AppleSupport\"').reset_index(drop=True)\n",
    "\n",
    "#remove @ and # words\n",
    "ibnd['text'] = ibnd['text'].apply(lambda x : re.sub('[@#][0-9a-zA-Z_]+', '', x))\n",
    "\n",
    "#Remove URLs\n",
    "ibnd['text'] = ibnd['text'].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '', x))\n",
    "\n",
    "#Removing new line character\n",
    "ibnd['text'] = ibnd['text'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "\n",
    "\n",
    "#Remove punctuation, stop words and lemmatize\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "text_clean = [clean(text).split() for text in ibnd['text'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.045*\"app\" + 0.024*\"help\" + 0.020*\"can’t\" + 0.019*\"update\" + 0.018*\"message\" + 0.016*\"iphone\" + 0.016*\"apps\" + 0.014*\"please\" + 0.013*\"open\" + 0.013*\"download\"'), (1, '0.114*\"i️\" + 0.072*\"fix\" + 0.024*\"letter\" + 0.023*\"please\" + 0.023*\"type\" + 0.017*\"glitch\" + 0.017*\"keyboard\" + 0.016*\"it\" + 0.015*\"issue\" + 0.015*\"this\"'), (2, '0.072*\"io\" + 0.053*\"iphone\" + 0.042*\"battery\" + 0.041*\"update\" + 0.026*\"phone\" + 0.025*\"6\" + 0.024*\"11\" + 0.023*\"since\" + 0.016*\"new\" + 0.016*\"7\"'), (3, '0.051*\"apple\" + 0.037*\"iphone\" + 0.029*\"x\" + 0.017*\"watch\" + 0.014*\"help\" + 0.014*\"phone\" + 0.013*\"get\" + 0.012*\"new\" + 0.011*\"store\" + 0.011*\"one\"'), (4, '0.034*\"photo\" + 0.031*\"macbook\" + 0.022*\"icloud\" + 0.021*\"mac\" + 0.021*\"ipad\" + 0.021*\"sierra\" + 0.021*\"pro\" + 0.020*\"high\" + 0.012*\"help\" + 0.011*\"picture\"'), (5, '0.064*\"phone\" + 0.037*\"keep\" + 0.029*\"screen\" + 0.023*\"time\" + 0.022*\"iphone\" + 0.019*\"update\" + 0.013*\"new\" + 0.013*\"every\" + 0.012*\"it’s\" + 0.011*\"even\"'), (6, '0.042*\"apple\" + 0.029*\"music\" + 0.028*\"email\" + 0.020*\"id\" + 0.018*\"account\" + 0.012*\"password\" + 0.012*\"song\" + 0.011*\"month\" + 0.010*\"number\" + 0.010*\"help\"'), (7, '0.065*\"work\" + 0.046*\"working\" + 0.031*\"call\" + 0.027*\"wifi\" + 0.024*\"doesn’t\" + 0.015*\"bluetooth\" + 0.014*\"phone\" + 0.014*\"turn\" + 0.012*\"isn’t\" + 0.011*\"button\"'), (8, '0.039*\"need\" + 0.029*\"phone\" + 0.028*\"y’all\" + 0.026*\"fix\" + 0.019*\"shit\" + 0.018*\"u\" + 0.018*\"wrong\" + 0.016*\"going\" + 0.015*\"what’s\" + 0.015*\"check\"'), (9, '0.050*\"question\" + 0.041*\"mark\" + 0.030*\"de\" + 0.029*\"box\" + 0.019*\"seeing\" + 0.018*\"que\" + 0.016*\"bar\" + 0.016*\"wtf\" + 0.015*\"la\" + 0.014*\"love\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#Term dictionary\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "\n",
    "#Document-Term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in text_clean]\n",
    "\n",
    "# Lda model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.115*\"iphone\" + 0.029*\"x\" + 0.025*\"7\" + 0.024*\"plus\" + 0.022*\"issue\" + 0.021*\"8\" + 0.020*\"6\" + 0.019*\"screen\" + 0.017*\"anyone\" + 0.014*\"problem\"'), (1, '0.066*\"time\" + 0.052*\"every\" + 0.043*\"screen\" + 0.027*\"phone\" + 0.017*\"“i”\" + 0.016*\"keep\" + 0.014*\"lock\" + 0.013*\"home\" + 0.010*\"go\" + 0.010*\"“it”\"'), (2, '0.046*\"app\" + 0.038*\"apple\" + 0.030*\"music\" + 0.024*\"can’t\" + 0.016*\"work\" + 0.016*\"call\" + 0.013*\"download\" + 0.012*\"store\" + 0.012*\"doesn’t\" + 0.011*\"apps\"'), (3, '0.048*\"watch\" + 0.031*\"de\" + 0.025*\"apple\" + 0.019*\"que\" + 0.019*\"charger\" + 0.017*\"hell\" + 0.016*\"told\" + 0.016*\"la\" + 0.012*\"3\" + 0.012*\"se\"'), (4, '0.105*\"i️\" + 0.067*\"fix\" + 0.022*\"phone\" + 0.022*\"letter\" + 0.021*\"type\" + 0.020*\"question\" + 0.019*\"please\" + 0.018*\"shit\" + 0.018*\"going\" + 0.017*\"box\"'), (5, '0.038*\"macbook\" + 0.027*\"glitch\" + 0.026*\"sierra\" + 0.025*\"pro\" + 0.025*\"high\" + 0.018*\"ipad\" + 0.017*\"mac\" + 0.017*\"working\" + 0.014*\"keyboard\" + 0.013*\"explain\"'), (6, '0.026*\"help\" + 0.024*\"apple\" + 0.022*\"phone\" + 0.017*\"iphone\" + 0.015*\"please\" + 0.014*\"get\" + 0.014*\"email\" + 0.013*\"photo\" + 0.011*\"new\" + 0.011*\"icloud\"'), (7, '0.115*\"battery\" + 0.032*\"hour\" + 0.031*\"io\" + 0.028*\"life\" + 0.026*\"iphone\" + 0.020*\"10\" + 0.017*\"drain\" + 0.017*\"minute\" + 0.016*\"charge\" + 0.015*\"6\"'), (8, '0.066*\"update\" + 0.057*\"io\" + 0.050*\"phone\" + 0.027*\"since\" + 0.026*\"new\" + 0.024*\"iphone\" + 0.019*\"11\" + 0.018*\"fix\" + 0.016*\"updated\" + 0.013*\"apps\"'), (9, '0.039*\"turn\" + 0.039*\"wifi\" + 0.023*\"bluetooth\" + 0.020*\"play\" + 0.020*\"turning\" + 0.019*\"stop\" + 0.018*\"fucking\" + 0.013*\"control\" + 0.013*\"playing\" + 0.012*\"keep\"')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      11356\n",
       "6       9372\n",
       "4       6234\n",
       "2       4099\n",
       "100     3278\n",
       "0       3002\n",
       "7       2332\n",
       "1       1635\n",
       "5       1321\n",
       "9       1038\n",
       "3        855\n",
       "dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the topic for each tweet\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "topics = []\n",
    "\n",
    "def get_topic(bow):\n",
    "    probs = ldamodel.get_document_topics(dictionary.doc2bow(bow))\n",
    "    topic = sorted(probs, key=lambda x : x[1], reverse=True)[0]\n",
    "    return topic[0] if topic[1]>threshold else 100\n",
    "\n",
    "for bow in text_clean:\n",
    "    topics.append(get_topic(bow))\n",
    "    \n",
    "pd.Series(topics).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
